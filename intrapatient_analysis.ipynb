{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the intrapatient analysis csv from the longitudinal test csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_consecutive_samples(row):\n",
    "    \"\"\"\n",
    "    Generate oversampled data based on the scandates and label.\n",
    "    Each consecutive sequence of scandates is created.\n",
    "    The label is assigned based on the last scan date in the combination.\n",
    "    \"\"\"\n",
    "    scandates = row['scandate'].split('-')\n",
    "    label = row['label']\n",
    "    samples = []\n",
    "\n",
    "    for start in range(len(scandates)):\n",
    "        for end in range(start + 1, len(scandates) + 1):\n",
    "            # Create a new sample with the consecutive scandates and the label\n",
    "            new_scandate = '-'.join(scandates[start:end])\n",
    "            new_label = label if scandates[-1] in new_scandate else 0\n",
    "            samples.append({'pat_id': row['pat_id'], 'scandate': new_scandate, 'label': new_label})\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Load test csv\n",
    "file_path = '/csvs/longitudinal_test.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "consecutive_oversampled_data = pd.DataFrame([sample for _, row in df.iterrows() for sample in generate_consecutive_samples(row)])\n",
    "\n",
    "# Save the oversampled dataframe to a CSV file\n",
    "output_file_path = '/analysis_csvs/intrapatient_analysis_test.csv' \n",
    "consecutive_oversampled_data.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the model output csv and intrapatient analysis csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results_df = pd.read_csv(\"/csvs/infer_test.csv\")\n",
    "scandate_data_df = pd.read_csv(\"/analysis_csvs/intrapatient_analysis_test.csv\")\n",
    "\n",
    "concatenated_df = pd.concat([scandate_data_df, validation_results_df], axis=1)\n",
    "scandate_data = pd.read_csv(\"/analysis_csvs/intrapatient_analysis_test.csv\")\n",
    "validation_data = concatenated_df.copy()\n",
    "\n",
    "# Merge the 'scandate' column from scandate_data into validation_data on 'pat_id'\n",
    "validation_results_with_scandate = validation_data.merge(\n",
    "    scandate_data[['pat_id', 'scandate']],\n",
    "    on='pat_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Adding a new column 'num_scans' to count the number of scandates\n",
    "validation_results_with_scandate['num_scans'] = validation_results_with_scandate['scandate'].apply(lambda x: len(x.split('-')))\n",
    "validation_results_with_scandate.to_csv(\"/analysis_csvs/intrapatient_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the intrapatient analysis results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "plt.style.use(['nature'])\n",
    "from confidenceinterval import roc_auc_score as auc_ci, accuracy_score as acc_ci, f1_score as f1_ci\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, balanced_accuracy_score, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "plt.rcParams.update({\n",
    "        'figure.dpi': '400' ,# Default DPI value\n",
    "})\n",
    "\n",
    "## UTIL FUNCTIONS \n",
    "def calculate_f1_scores(df):\n",
    "    '''\n",
    "    Calculate F1 scores for a given dataframe\n",
    "    - Group by patient id\n",
    "    - For each patient, calculate f1 scores and return the mean\n",
    "    '''\n",
    "    grouped = df.groupby('pat_id')\n",
    "\n",
    "    f1_scores = {}\n",
    "    for name, group in grouped:\n",
    "        score = f1_score(group['GT'], group['ModelPredictions'], average=\"weighted\")\n",
    "        f1_scores[name] = score\n",
    "\n",
    "    return np.mean(list(f1_scores.values()))\n",
    "\n",
    "\n",
    "def calculate_f1_scores_with_ci(df, n_bootstraps=100, ci=95):\n",
    "    '''Calculate F1 scores with confidence intervals'''\n",
    "    grouped = df.groupby('pat_id')\n",
    "    f1_scores = {}\n",
    "    for name, group in grouped:\n",
    "        score = f1_score(group['GT'], group['ModelPredictions'], average=\"macro\")\n",
    "        f1_scores[name] = score\n",
    "    original_mean = np.mean(list(f1_scores.values()))\n",
    "\n",
    "    bootstrap_means = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_df = df.sample(n=len(df), replace=True)\n",
    "        resampled_grouped = resampled_df.groupby('pat_id')\n",
    "        resampled_f1_scores = []\n",
    "        for name, group in resampled_grouped:\n",
    "            score = f1_score(group['GT'], group['ModelPredictions'], average=\"macro\")\n",
    "            resampled_f1_scores.append(score)\n",
    "        bootstrap_means.append(np.mean(resampled_f1_scores))\n",
    "\n",
    "    lower_bound = np.percentile(bootstrap_means, (100 - ci) / 2)\n",
    "    upper_bound = np.percentile(bootstrap_means, 100 - (100 - ci) / 2)\n",
    "\n",
    "    return (original_mean, lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "def get_metircs_data(df):\n",
    "    \"\"\"\n",
    "    - filter by number of scans\n",
    "    - Collect metrics based on number of scans\n",
    "    returns dataframe indexed by number of scans\n",
    "    \"\"\"\n",
    "    metrics_data = {\n",
    "        'f1_score': [],\n",
    "        'f1_lower_ci': [], 'f1_upper_ci': [],\n",
    "        'n_cases': [], 'n_scans': [], \"n_true_prediction\": [],\n",
    "        \"num_positive_GT\":[]\n",
    "    }\n",
    "    for num in range(1, 13):\n",
    "        \n",
    "        sample_df = df[df.num_scans == num]\n",
    "        if sample_df[\"GT\"].nunique() > 1:        \n",
    "            # calculate f1 score for filtered number of scan\n",
    "            f1_mean, lower_bound, upper_bound = calculate_f1_scores_with_ci(sample_df)\n",
    "            metrics_data['f1_score'].append(f1_mean)\n",
    "            metrics_data[\"f1_lower_ci\"].append(lower_bound)\n",
    "            metrics_data[\"f1_upper_ci\"].append(upper_bound)\n",
    "            metrics_data['n_cases'].append(len(sample_df))\n",
    "            metrics_data['n_scans'].append(num)\n",
    "            metrics_data[\"n_true_prediction\"].append(len(sample_df[sample_df[\"GT\"] == sample_df[\"ModelPredictions\"]]))\n",
    "            metrics_data[\"num_positive_GT\"].append(len(sample_df[sample_df[\"GT\"]==1]))\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    metrics_df.set_index(\"n_scans\", inplace=True)\n",
    "    metrics_df[\"Tot(Pos)\"]= [f\"{total}\\n({true})\" for total, true in zip(metrics_df['n_cases'], metrics_df['num_positive_GT'])]\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def make_f1_plot(metrics_df):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    error = [metrics_df['f1_score'] - metrics_df['f1_lower_ci'], metrics_df['f1_upper_ci'] - metrics_df['f1_score']]\n",
    "    dot_color = \"#B12A90\"  \n",
    "    error_color = 'lightgray'  \n",
    "    error_lw = 3 \n",
    "    capsize = 5 \n",
    "    ax.errorbar(metrics_df.index, metrics_df['f1_score'], yerr=error, fmt='o', color=dot_color, ecolor=error_color, \n",
    "                elinewidth=error_lw, capsize=capsize, linestyle='-', linewidth=2, markersize=5, markeredgecolor=dot_color,\n",
    "                markerfacecolor=dot_color, markeredgewidth=2, label='F1 Score')\n",
    "    ax.set_xlabel('Number of Scans')\n",
    "    # ax.set_ylabel('F1 Score')\n",
    "    # ax.set_title('F1 Scores per Number of Scans')\n",
    "    ax.set_xticks(metrics_df.index)\n",
    "    ax.set_xticklabels(metrics_df.index)\n",
    "    ax.spines['top'].set_visible(False)   \n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_intrapatient = pd.read_csv(\"/analysis_csvs/intrapatient_analysis.csv\")\n",
    "intrapatient_metrics = get_metircs_data(test_intrapatient)\n",
    "_ = make_f1_plot(intrapatient_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
